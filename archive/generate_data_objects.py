#!/usr/bin/env python3
"""
Generate Trimble XChange Data Objects from prompt packets using an LLM.

This script reads prompt packets (generated by generate_prompt_packets.py) and
calls an LLM to generate the corresponding C# XChange DataObject code.

Supports multiple LLM providers:
- Anthropic Claude (default)
- OpenAI GPT-4

Usage:
    python generate_data_objects.py <ir_name>
    python generate_data_objects.py servicefusion
    python generate_data_objects.py servicefusion --schema CustomerDataObject
    python generate_data_objects.py servicefusion --provider openai

Environment Variables:
    ANTHROPIC_API_KEY - API key for Anthropic Claude
    OPENAI_API_KEY - API key for OpenAI

Output:
    Creates C# files in: ./generated/<ir_name>/<SchemaName>DataObject.cs
"""

import argparse
import json
import os
import sys
from pathlib import Path
from typing import Any, Protocol

# Load .env file if present
def load_dotenv():
    """Load environment variables from .env file."""
    env_path = Path(__file__).parent / ".env"
    if env_path.exists():
        with open(env_path) as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith("#") and "=" in line:
                    key, _, value = line.partition("=")
                    key = key.strip()
                    value = value.strip().strip("\"'")
                    if key and value:
                        os.environ.setdefault(key, value)

load_dotenv()


class LLMProvider(Protocol):
    """Protocol for LLM providers."""

    def generate(self, prompt: str, system: str | None = None) -> str:
        """Generate a response from the LLM."""
        ...


class AnthropicProvider:
    """Anthropic Claude provider."""

    def __init__(self, api_key: str, model: str = "claude-3-5-haiku-20241022"):
        self.api_key = api_key
        self.model = model
        self._client = None

    @property
    def client(self):
        if self._client is None:
            try:
                import anthropic
                self._client = anthropic.Anthropic(api_key=self.api_key)
            except ImportError:
                print("Error: anthropic package not installed.", file=sys.stderr)
                print("Install with: pip install anthropic", file=sys.stderr)
                sys.exit(1)
        return self._client

    def generate(self, prompt: str, system: str | None = None) -> str:
        message = self.client.messages.create(
            model=self.model,
            max_tokens=4096,
            system=system or "You are a C# code generator specializing in Trimble XChange DataObjects.",
            messages=[{"role": "user", "content": prompt}],
        )
        return message.content[0].text


class OpenAIProvider:
    """OpenAI GPT provider."""

    def __init__(self, api_key: str, model: str = "gpt-4o"):
        self.api_key = api_key
        self.model = model
        self._client = None

    @property
    def client(self):
        if self._client is None:
            try:
                import openai
                self._client = openai.OpenAI(api_key=self.api_key)
            except ImportError:
                print("Error: openai package not installed.", file=sys.stderr)
                print("Install with: pip install openai", file=sys.stderr)
                sys.exit(1)
        return self._client

    def generate(self, prompt: str, system: str | None = None) -> str:
        messages = []
        if system:
            messages.append({"role": "system", "content": system})
        messages.append({"role": "user", "content": prompt})

        response = self.client.chat.completions.create(
            model=self.model,
            messages=messages,
            max_tokens=4096,
        )
        return response.choices[0].message.content


class DryRunProvider:
    """Dry-run provider that doesn't call any API."""

    def generate(self, prompt: str, system: str | None = None) -> str:
        return "// Dry run - no code generated"


def load_json(path: Path) -> dict:
    """Load JSON from a file."""
    with open(path) as f:
        return json.load(f)


def format_fields_section(fields: list[dict], indent: str = "  ") -> list[str]:
    """Format a list of fields for the prompt."""
    lines = []
    for field in fields:
        attrs = []
        if field.get("required"):
            attrs.append("required")
        if field.get("nullable"):
            attrs.append("nullable")
        if field.get("deprecated"):
            attrs.append("deprecated")

        attr_str = f" [{', '.join(attrs)}]" if attrs else ""
        type_str = field["csharp_type"]
        if field.get("nullable") and not type_str.endswith("?"):
            type_str += "?"

        lines.append(f"{indent}{field['json_name']}: {type_str}{attr_str}")
        if field.get("description"):
            lines.append(f"{indent}  Description: {field['description']}")
        lines.append("")
    return lines


def build_prompt_from_grouped_packet(packet: dict) -> str:
    """Build the LLM prompt from a GROUPED prompt packet (with nested types)."""
    metadata = packet["metadata"]
    parent = packet["parent"]
    nested_types = packet.get("nested_types", [])
    generation = packet["generation"]

    lines = []

    # Instructions
    lines.append(generation["instructions"])
    lines.append("")

    # Parent schema
    lines.append("=" * 60)
    lines.append(f"PARENT DATAOBJECT: {metadata['data_object_name']}")
    lines.append("=" * 60)
    lines.append("")
    lines.append(f"API Name: {metadata['ir_name']}")
    lines.append(f"Schema ID: {parent['schema_id']}")
    lines.append(f"Description: {parent['description'] or 'No description'}")
    if parent.get("primary_key_field"):
        lines.append(f"Primary Key: {parent['primary_key_field']}")
    lines.append("")

    lines.append("FIELDS:")
    lines.append("-" * 40)
    lines.extend(format_fields_section(parent["fields"]))

    # Nested types
    if nested_types:
        lines.append("")
        lines.append("=" * 60)
        lines.append(f"NESTED TYPES (define in same file)")
        lines.append("=" * 60)

        for nested in nested_types:
            lines.append("")
            lines.append(f"CLASS: {nested['name']}")
            lines.append("-" * 40)
            if nested.get("description"):
                lines.append(f"Description: {nested['description']}")
            lines.append("")
            lines.append("FIELDS:")
            lines.extend(format_fields_section(nested["fields"], indent="  "))

    # Example
    lines.append("")
    lines.append("EXAMPLE CODE PATTERN:")
    lines.append("-" * 40)
    lines.append(generation["example_code"])

    return "\n".join(lines)


def build_prompt_from_packet(packet: dict) -> str:
    """Build the full LLM prompt from a prompt packet."""
    # Check if this is a grouped packet
    if packet.get("metadata", {}).get("is_grouped"):
        return build_prompt_from_grouped_packet(packet)

    # Original flat packet format
    metadata = packet["metadata"]
    fields = packet["fields"]
    related = packet.get("related_schemas", {})
    generation = packet["generation"]

    lines = []

    # Instructions
    lines.append(generation["instructions"])
    lines.append("")

    # Schema information
    lines.append("=" * 60)
    lines.append(f"SCHEMA: {metadata['data_object_name']}")
    lines.append("=" * 60)
    lines.append("")
    lines.append(f"API Name: {metadata['ir_name']}")
    lines.append(f"Schema ID: {metadata['schema_id']}")
    lines.append(f"Description: {metadata['description'] or 'No description'}")
    if metadata.get("primary_key_field"):
        lines.append(f"Primary Key: {metadata['primary_key_field']}")
    lines.append("")

    # Fields
    lines.append("FIELDS:")
    lines.append("-" * 40)
    lines.extend(format_fields_section(fields))

    # Related schemas
    if related:
        lines.append("")
        lines.append("RELATED TYPES (for nested objects):")
        lines.append("-" * 40)
        for rel_name, rel_info in related.items():
            lines.append(f"\n  {rel_name}:")
            if rel_info.get("description"):
                lines.append(f"    Description: {rel_info['description']}")
            for rf in rel_info.get("fields", [])[:10]:
                rf_attrs = []
                if rf.get("required"):
                    rf_attrs.append("required")
                if rf.get("nullable"):
                    rf_attrs.append("nullable")
                if rf.get("write_only"):
                    rf_attrs.append("PII")
                attr_str = f" [{', '.join(rf_attrs)}]" if rf_attrs else ""
                lines.append(f"      {rf['json_name']}: {rf['csharp_type']}{attr_str}")
            if len(rel_info.get("fields", [])) > 10:
                lines.append(f"      ... and {len(rel_info['fields']) - 10} more fields")

    # Example
    lines.append("")
    lines.append("EXAMPLE CODE PATTERN:")
    lines.append("-" * 40)
    lines.append(generation["example_code"])

    return "\n".join(lines)


def extract_code_from_response(response: str) -> str:
    """Extract C# code from LLM response, handling markdown code blocks."""
    # Check if response contains markdown code blocks
    if "```csharp" in response:
        # Extract code between ```csharp and ```
        start = response.find("```csharp") + len("```csharp")
        end = response.find("```", start)
        if end > start:
            return response[start:end].strip()

    if "```cs" in response:
        start = response.find("```cs") + len("```cs")
        end = response.find("```", start)
        if end > start:
            return response[start:end].strip()

    if "```" in response:
        start = response.find("```") + 3
        # Skip language identifier if present
        newline = response.find("\n", start)
        if newline > start and newline - start < 20:
            start = newline + 1
        end = response.find("```", start)
        if end > start:
            return response[start:end].strip()

    # No code blocks, return as-is (assume it's all code)
    return response.strip()


def get_provider(provider_name: str, model: str | None = None, dry_run: bool = False) -> LLMProvider:
    """Get the appropriate LLM provider."""
    if dry_run:
        return DryRunProvider()

    if provider_name == "anthropic":
        api_key = os.environ.get("ANTHROPIC_API_KEY")
        if not api_key:
            print("Error: ANTHROPIC_API_KEY environment variable not set", file=sys.stderr)
            print("Set it in .env file or environment", file=sys.stderr)
            sys.exit(1)
        return AnthropicProvider(api_key, model=model) if model else AnthropicProvider(api_key)

    elif provider_name == "openai":
        api_key = os.environ.get("OPENAI_API_KEY")
        if not api_key:
            print("Error: OPENAI_API_KEY environment variable not set", file=sys.stderr)
            print("Set it in .env file or environment", file=sys.stderr)
            sys.exit(1)
        return OpenAIProvider(api_key, model=model) if model else OpenAIProvider(api_key)

    else:
        print(f"Error: Unknown provider '{provider_name}'", file=sys.stderr)
        print("Supported providers: anthropic, openai", file=sys.stderr)
        sys.exit(1)


def main():
    parser = argparse.ArgumentParser(
        description="Generate XChange DataObjects from prompt packets using an LLM.",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=__doc__,
    )
    parser.add_argument(
        "ir_name",
        help="Name of the IR (matches prompt_packets directory)",
    )
    parser.add_argument(
        "--schema",
        help="Generate for a specific schema only (e.g., 'CustomerDataObject')",
    )
    parser.add_argument(
        "--provider",
        choices=["anthropic", "openai"],
        default="anthropic",
        help="LLM provider to use (default: anthropic)",
    )
    parser.add_argument(
        "--model",
        default=None,
        help="Model to use (default: claude-3-5-haiku-20241022 for anthropic, gpt-4o for openai)",
    )
    parser.add_argument(
        "--packets-dir",
        type=Path,
        default=None,
        help="Directory containing prompt packets (default: ./prompt_packets/<ir_name>)",
    )
    parser.add_argument(
        "--output-dir",
        type=Path,
        default=None,
        help="Output directory for generated code (default: ./generated/<ir_name>)",
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Show prompts without calling LLM",
    )
    parser.add_argument(
        "--show-prompt",
        action="store_true",
        help="Print the prompt being sent to the LLM",
    )
    parser.add_argument(
        "--limit",
        type=int,
        default=None,
        help="Limit number of schemas to process",
    )

    args = parser.parse_args()

    # Determine paths
    script_dir = Path(__file__).parent
    packets_dir = args.packets_dir or (script_dir / "prompt_packets" / args.ir_name)
    output_dir = args.output_dir or (script_dir / "generated" / args.ir_name)

    if not packets_dir.exists():
        print(f"Error: Prompt packets directory not found: {packets_dir}", file=sys.stderr)
        print(f"\nRun generate_prompt_packets.py first:", file=sys.stderr)
        print(f"  python generate_prompt_packets.py {args.ir_name}", file=sys.stderr)
        sys.exit(1)

    # Get list of packets to process
    packet_files = sorted(packets_dir.glob("*.json"))

    if args.schema:
        # Filter to specific schema
        target = args.schema.replace(".json", "")
        if not target.endswith("DataObject"):
            target = f"{target}DataObject"
        packet_files = [f for f in packet_files if target in f.stem]
        if not packet_files:
            print(f"Error: No packet matching '{args.schema}' found", file=sys.stderr)
            sys.exit(1)

    if args.limit:
        packet_files = packet_files[:args.limit]

    print(f"Processing {len(packet_files)} prompt packets...")

    # Get LLM provider
    provider = get_provider(args.provider, model=args.model, dry_run=args.dry_run)

    # Create output directory
    if not args.dry_run:
        output_dir.mkdir(parents=True, exist_ok=True)

    # Process each packet
    generated = 0
    errors = 0

    for packet_file in packet_files:
        packet = load_json(packet_file)
        name = packet["metadata"]["data_object_name"]
        print(f"\n{'=' * 60}")
        print(f"Processing: {name}")
        print(f"{'=' * 60}")

        # Build prompt
        prompt = build_prompt_from_packet(packet)

        if args.show_prompt or args.dry_run:
            print("\n--- PROMPT ---")
            print(prompt[:2000])
            if len(prompt) > 2000:
                print(f"\n... ({len(prompt) - 2000} more characters)")
            print("--- END PROMPT ---\n")

        if args.dry_run:
            print(f"  Would generate: {name}.cs")
            generated += 1
            continue

        # Call LLM
        try:
            print(f"  Calling {args.provider}...")
            response = provider.generate(prompt)

            # Extract code
            code = extract_code_from_response(response)

            # Write output
            output_path = output_dir / f"{name}.cs"
            with open(output_path, "w") as f:
                f.write(code)

            print(f"  Generated: {output_path}")
            generated += 1

        except Exception as e:
            print(f"  Error: {e}", file=sys.stderr)
            errors += 1

    # Summary
    print(f"\n{'=' * 60}")
    print(f"SUMMARY")
    print(f"{'=' * 60}")
    print(f"Generated: {generated}")
    print(f"Errors: {errors}")
    if not args.dry_run and generated > 0:
        print(f"Output directory: {output_dir}")


if __name__ == "__main__":
    main()
